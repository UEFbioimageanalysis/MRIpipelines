# MRIpipelines
This repository contains example pipelines to run different T1w-MRI processing tools in an HPC-like environment. 
These are intended to serve as examples how to do process anatomical MRI data on the UEF bioinformatics servers, but might be useful for also others not using UEF servers.
These pipelines assume that the server runs slurm workload manager. They also utilize snakemake workflow management system.

Jussi Tohka, Teemu Mäki-Maunus, Raimo Salo, and Ali Farki have contributed to these pipelines.

The aim of this project is to streamline the analysis pipeline for BIDS-formatted files. By utilizing Slurm, large amounts of resources can be allocated to resource-intensive programs. Snakemake is used as a tool to enable an easy-to-understand and efficient form of parallel computing. The main image analysis is carried out using software such as FreeSurfer and CAT12, which take in variables and produce derivative data.

Please cite the relevant pipelines according to their instructions. Note the limitations/conditions of use of these pipelines.

------------------------------------------------------------------------------------------------------------------------------------------
Snakemake Code Overview:

Snakemake operates with:

- Python and Shell Scripts: Snakemake combines Python scripting with shell commands.
- Rule-based Structure:

Snakemake uses rules to define tasks.

The runAll rule is activated first unless specified otherwise.

Each rule has an input and output, where the output produces new files or data.

If a rule's input cannot be found, Snakemake will search for a rule whose output matches that input.

Parameters and Shell Commands: Each rule consists of parameters (params) and scripts (usually shell commands) to generate the output.

- Wildcards: 

Wildcards are dynamic variables generated by expand commands. If only one wildcard is used, the wildcard stem suffices. If multiple wildcards are present, use the names specified in expand(stem=list).

- Expand Command: 

The expand function ensures that the rule processes one file at a time.

------------------------------------------------------------------------------------------------------------------------------------------
Activating Snakemake:

To run Snakemake, use the following example command, adding any extra variables, such as cores:

snakemake --snakefile snakefiles_synthseq --jobs=20 --cores=10 --default-resources mem_mb=5000 disk_mb=5000 runtime=20 --latency-wait=120 --use-conda

If no --snakefile is specified, Snakemake will search for a file named Snakefile by default.

To activate Snakemake, either run it from the directory with the Snakefile or specify the filename using --snakefile.

You can add additional variables like the number of cores or memory limits as shown.

------------------------------------------------------------------------------------------------------------------------------------------
Snakemake Configuration File (config.yaml):

The configuration file works as a shortcut for frequently used parameters and can be used to load environment modules via envmodules.

Cluster Settings: This section defines how Snakemake submits jobs to Slurm. Parameters like memory, runtime, and cores can be specified here. In essence you write here the code you otherwise would have to use every time you activate the snakemake.

Default Resources:

This section defines the default resources for SLURM jobs, including:

cores: Specifies the maximum number of CPU cores allocated per job.

mem_mb: The memory allocated for each job, in megabytes.

disk_mb: The storage allocated for each job, in megabytes.

runtime: The maximum runtime allowed per job, in minutes.

Additional Information:

cores: Specifies the number of CPU cores available for local computation (on the machine running Snakemake).
jobs: Limits the maximum number of simultaneous jobs that can be submitted to SLURM.
latency-wait: Specifies how long Snakemake should wait (in seconds) for output files to appear after job completion. This is useful for synchronizing file system operations in cases of potential latency issues.

For more information on Snakemake, visit the official documentation at https://snakemake.readthedocs.io.

------------------------------------------------------------------------------------------------------------------------------------------
Files Needed to Install:

Snakemake:
Install Miniconda (if not already installed):
Download and run the installer by typing:
sh Miniconda3-latest-Linux-x86_64.sh

Install Mamba (a faster alternative to Conda):
Run the following command to install Mamba:
conda install -n base -c conda-forge mamba

Install Snakemake:
Use Mamba to create an environment and install Snakemake:
mamba create -c conda-forge -c bioconda -n snakemake snakemake
Install Additional Modules (if needed):

If you need more Python modules, you can install them by creating a new environment. For example:

mamba create -c conda-forge -c bioconda -n ENV python=3.10 snakemake dipy numpy pandas scipy

------------------------------------------------------------------------------------------------------------------------------------------

SPM12:
Download SPM12:
Go to the official download page: SPM12 Download. https://www.fil.ion.ucl.ac.uk/spm/software/download/
Install SPM12:
After downloading the zip file, unzip it and move it to your MATLAB folder.

Set up SPM12 in MATLAB:
Open MATLAB, then add the SPM12 directory to your MATLAB path by typing the following in the MATLAB prompt:
matlab
addpath('C:\Users\<login>\Documents\MATLAB\spm12')
savepath

------------------------------------------------------------------------------------------------------------------------------------------

CAT12:

The Snakemake workflow for CAT12 requires you to manually specify the location of the CAT12 toolbox in the Snakemake file.
Example: In the Snakefile_CAT_OASIS_BIDS, change the path "/home/matlab/spm12/toolbox/cat12" to match the location where you installed CAT12.

------------------------------------------------------------------------------------------------------------------------------------------

Freesurfer and SynthSeg:
Freesurfer and the SynthSeg addition are pre-installed on the Kudos system.
To activate Freesurfer, use the following command:
module load freesurfer

MATLAB:
MATLAB is also pre-installed on Kudos. You can activate it in the same way as Freesurfer:
module load matlab

------------------------------------------------------------------------------------------------------------------------------------------

Pipelines:
mri_synthseg --i {input.source} --o {output.segfile} --vol {output.csvfile} \
                     --qc {output.qcfile} --resample {output.resamplefile} --threads {threads} --cpu

Description: This command runs SynthSeg, an automated brain segmentation tool. It segments an MRI input file into various anatomical structures.

Parameters:
--i {input.source}: Specifies the input MRI file.
--o {output.segfile}: Outputs the segmented brain image.
--vol {output.csvfile}: Outputs a CSV file containing volume measurements of the segmented regions.
--qc {output.qcfile}: Produces a quality control output for the segmentation.
--resample {output.resamplefile}: Outputs the resampled image.
--threads {threads}: Sets the number of CPU threads to be used.
--cpu: Forces the job to run on CPU rather than GPU.


{params.cat12_dir}/cat_batch_cat.sh {input.t1} -p 1 {params.fg} --matlab {params.matlab} --defaults {params.default} {params.no_surf} {params.rp} --bids_folder {params.bids_folder_cross} --logdir {params.log_folder}

Description: This command runs CAT12, a toolbox for automated MRI segmentation and morphometry analysis.

Parameters:
{params.cat12_dir}: Directory where CAT12 is located.
{input.t1}: Input T1-weighted MRI file.
-p 1: A flag used to specify processing steps (e.g., one specific pipeline).
{params.fg}: Foreground parameters (specific options passed to CAT12).
--matlab {params.matlab}: Specifies the path to MATLAB.
--defaults {params.default}: Specifies the default configuration file for CAT12.
{params.no_surf}: Parameter for surface reconstruction options.
{params.rp}: Option for additional preprocessing or quality checks.
--bids_folder {params.bids_folder_cross}: Specifies the output directory in BIDS format.
--logdir {params.log_folder}: Specifies the log directory.

recon-all -s {params.ses_id} -i {input.source} -sd {params.result_path} -cw256 -all
Description: This command runs Freesurfer’s recon-all tool for cortical surface reconstruction and brain segmentation from an MRI image.

Parameters:
-s {params.ses_id}: Subject or session ID.
-i {input.source}: Input MRI file.
-sd {params.result_path}: Output directory where the results will be stored.
-cw256: Resamples the input image to 256x256 voxels.
-all: Runs all the standard recon-all processing stages (skull stripping, segmentation, surface reconstruction, etc.).

recon-all -long {params.session_id} {params.template_id} -sd "../derivatives/freesurfer7.4.1/{params.subject_id}" -cw256 -all

Description: This command runs Freesurfer’s longitudinal pipeline for within-subject analysis across multiple time points.

Parameters:
-long {params.session_id} {params.template_id}: Runs the longitudinal analysis, using session-specific data and a template created by Freesurfer.
-sd {params.subject_id}: Specifies the output directory.
-cw256: Resamples the image to 256x256 voxels.
-all: Runs all Freesurfer processing steps.

recon-all -base {params.template_id} {params.sessions} -sd "../derivatives/freesurfer7.4.1/{params.subject_id}" -cw256 -all

Description: This command creates a template for longitudinal analysis using data from multiple sessions in Freesurfer.

Parameters:
-base {params.template_id}: Creates a base template from multiple sessions.
{params.sessions}: List of session IDs to include in the base template.
-sd {params.subject_id}: Output directory for the Freesurfer results.
-cw256: Resamples the image to 256x256 voxels.

-all: Runs all Freesurfer processing stages on the template.

